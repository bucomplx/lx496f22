{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j3aFniNj6dyf"
   },
   "source": [
    "# LX 496 / 796 Homework 2\n",
    "\n",
    "In this homework we will do some classification and categorization, and work through some Python.  We talked through a lot of these things in class, so much of this is kind of just review, but also provides some practice doing it yourself.\n",
    "\n",
    "**Using the \"autograder\":** The way this notebook is set up is that you read a bunch of stuff, and then periodically you will be prompted to answer a question.  The question might take the form of a question you answer in prose, or (more often) a question you answer in code. The code questions come with a test that will check your answer.  If you have it correct, it will tell you that the question \"passed.\"  So, you can be fairly confident as you go along that you got the code working.  You'll want to submit a notebook where things pass.\n",
    "\n",
    "**Submitting at the end:** When you are ready to submit, download the `.ipynb` file and then go to Gradescope and upload it there.  The autograder will run and double-check your answers.  In principle there may be some tests (\"hidden tests\") that Gradescope runs that you didn't have access to when you were working through it.\n",
    "\n",
    "**Getting started**: In order for the checking procedure to work, you need to run this cell below first.  It will download the autograder stuff and the tests.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "YO6rRkW7N_24"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Run this first, it makes the autograder go.\n",
    "# The \"%%capture\" line above keeps it from showing all the output\n",
    "# Install the autograder (otter-grader) and get the test files\n",
    "files = \"https://github.com/bucomplx/lx496f22/raw/main/assets/ipynb/hw2/tests.zip\"\n",
    "!pip install otter-grader && wget $files && unzip -o tests.zip\n",
    "# enable the otter test evaluator\n",
    "import otter\n",
    "grader = otter.Notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KJibPY2kPQYr"
   },
   "source": [
    "Before we get too far into this, let's try a question that the autograder can check.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "GLgly56K59B0"
   },
   "source": [
    "### q0 (test autograder) ###\n",
    "\n",
    "**Question:** In the code block below, change the `...` to be the number `4` and then execute the check.  It will fail.  Then change it to be the number `10` and it will pass.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q0\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "9YxbkeR3P0dy"
   },
   "outputs": [],
   "source": [
    "# First change the ... to 4 to see what failure looks like, then change to 10\n",
    "first_answer = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ReFYA5VPQEOc"
   },
   "source": [
    "Great.  So, that's how the autograder works.  Whenever you see `...` like that after `=`, that's a spot for you to fill something in.  The instructions will tell you what to fill in.  Actually, you might also have noticed that when you had the wrong answer in, the autograder revealed what was supposed to be there.  Specifically, you'll have seen `assert first_answer == 10` in there, which is `True` if `first_answer` is `10` and otherwise presents an error.  So, you can get clues to what it is looking for, if your test isn't passing.  Of course, usually, getting the test to pass will be more involved than just setting the number to be what it expects to see, but it is a source of hints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "akFFJIsRRBcb"
   },
   "source": [
    "# Sentiment analysis on movie reviews\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cXf7pxAxukxO"
   },
   "source": [
    "## Preparing the corpus data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fTsoa4ugUGuT"
   },
   "source": [
    "NLTK comes with a corpus of movie reviews, that has been segmented into positive and negative reviews. Using this, we will try to construct a machine that can guess whether a new review is positive or negative. (\"Sentiment analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-z86EQC1wPnu",
    "outputId": "6675b829-e99c-4ca1-8cad-711325648c27"
   },
   "outputs": [],
   "source": [
    "# make NLTK and the movie reviews corpus available\n",
    "import nltk\n",
    "from nltk.corpus import movie_reviews\n",
    "nltk.download('movie_reviews')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5DmVpyZpUp83"
   },
   "source": [
    "Different corpora are organized in different ways, but generally they are collections of individual files, often categorized. The movie reviews corpus has many files, one file per review, and categorized into those that are positive and those that are negative. The positive reviews are in the `pos` directory (folder) and the negative reviews are in the `neg` directory. (When writing out a filename, folders are indicated by a `/` character.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qVWY7rI-SpXI"
   },
   "source": [
    "**Activity**.  When we did the `from nltk.corpus import movie_reviews` line above, we effectively created an object called `movie_reviews` which is a corpus-type object and knows how to do a number of different things.  Among the things it knows how to do is tell you what different categories it has in it.  It can also tell you what files it contains (`fileids`), and some other things.  Below, try typing just `movie_reviews.` and stop after the `.`. You'll see a bunch of options come up; these are the the things the `movie_reviews` corpus knows how to do.  Continue then by typing `categories()` to ask it to tell you what categories it has."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "7yc542v6Lwvc"
   },
   "outputs": [],
   "source": [
    "# Below, type\n",
    "# movie_reviews.categories()\n",
    "# to see what categories this corpus has defined.\n",
    "# Conceptually, we are addressing the movie_reviews object and asking it\n",
    "# do something, specifically to run the categories() function for us.\n",
    "# Try stopping typing after you type the . and, after a moment, it should\n",
    "# pop up a list of the various things that movie_reviews knows how to do.\n",
    "# That's useful if you quite remember the name of the function, or are just\n",
    "# curious about other things you can do with the movie reviews corpus.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e-1oXCQRMLWm"
   },
   "source": [
    "When you looked at what `movie_reviews` knew how to do (that is, \"what methods are defined\" in Python parlance), which you got above by typing `movie_reviews.` and looking at the autocompletion popup, you saw `raw` and `categories` and `readme` and some other things.  The `readme` method gives you information about the corpus. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "DIWFnpPgYgJL"
   },
   "source": [
    "### q1 (find dataset year) ###\n",
    "\n",
    "**Question:** Get the corpus to tell you the readme information, in the same kind of way we got it to tell us what the categories were above.  Then use the information you find there to answer the question (fill in the year below).\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q1\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C3yQTYRXLWgX",
    "outputId": "16212716-40ff-4474-9ba3-70c6e2a2a6ef"
   },
   "outputs": [],
   "source": [
    "# Put a command below that will print out the readme information\n",
    "...\n",
    "\n",
    "# Question: what year was this (v2.0) dataset released?\n",
    "# So the autograder can check it, assign the proper year to v2_year\n",
    "# It should look like\n",
    "# v2_year = 1066\n",
    "# Note: 1066 is not the correct year\n",
    "v2_year = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T4WZ34qtYcUC"
   },
   "source": [
    "Now, back to the problem at hand. We now know what the categories are (you saw them back in the \"activity\" erlier), but let's print them in a nicer way.  First, we'll count how many there are by asking for the `len()` (length) of the list that `categories()` returned, and then we'll list them out, joined by commas between them. We can do this using `len()` and `format()` and `join()`. This should be kind of familiar, but make sure you understand how these lines work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ky0_CbV3UeEl",
    "outputId": "1b07e952-87d3-4bcb-ac5b-6e3748510d59"
   },
   "outputs": [],
   "source": [
    "print(\"There are {} categories\".format(len(movie_reviews.categories())))\n",
    "print(\"They are: \" + \", \".join(movie_reviews.categories()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "65T3R6gfc9sV"
   },
   "source": [
    "Nice!  That's much easier to read.  Another thing that the corpus can do is give it the names of all the files it contains. We can get that list with `fileids()`. That list is long though.  There are (as we'll see below) 2000 of them. So, printing all 2000 of them out with commas in between would just be an unreadable mess.  We don't need to see them all, we can just look at the first three to get a sense of what they look like. It's not a bad idea to just make sure that the things you're working with (like categories or fileids) look like what you expect them to look like. So this is a kind of sanity check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cIQSXJ3dVhtp",
    "outputId": "87cd84ae-0202-4986-caae-f5613ae7607f"
   },
   "outputs": [],
   "source": [
    "print(\"There are {} files\".format(len(movie_reviews.fileids())))\n",
    "print(\"The first 3 are: \" + \", \".join(movie_reviews.fileids()[:3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WXW50rPxWe0A"
   },
   "source": [
    "Once we did the `...import movie_reviews` step, `movie_reviews` became a known corpus object, that we can interact with.  We can tell the corpus object to give us its categories, as above, with `categories()`, and we can ask it to give us its filenames with `fileids()`. If we want just the filenames for a single category, we can specify it like this: `fileids(categories='pos')` or (if you want the filenames from several categories) `fileids(categories=['neg', 'pos'])`. The `categories=` parameter can take either a string (naming a category) or a list of strings (each of which names a category).\n",
    "\n",
    "The code below will go through the categories, and, for each category, it will call the current category `c` and then execute the indented block.  The first line gets the fileids for all the reviews in category `c` (whatever `c` is on this iteration), and then tells us how many there are and what the first 3 fileids are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MXAheuVTV--m",
    "outputId": "60658334-1a6c-4015-a040-9bd4a202f399"
   },
   "outputs": [],
   "source": [
    "for c in movie_reviews.categories():\n",
    "  ids = movie_reviews.fileids(categories=c)\n",
    "  print(\"There are {} reviews in category {}.\".format(len(ids), c))\n",
    "  print(\"The first 3 files are: \" + ', '.join(ids[:3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Me8uFQRgXdTH"
   },
   "source": [
    "The hypothesis we will pursue first is that we can guess whether a review is positive or negative based on what words it contains. Roughly speaking, a review that contains the word \"boring\" is probably negative, and one that contains the word \"exciting\" is probably positive.\n",
    "\n",
    "So, let's try to work that out.\n",
    "\n",
    "These reviews are just text, they have not been processed except to split the text up into words.  So one thing we want to do right away is convert all the words to lowercase, so we don't consider a word like \"boring\" to be two different words depending on whether it is at the beginning of a sentence or in the middle of a sentence.\n",
    "\n",
    "The second thing we want to do is to remove all the super-common/grammatical words and punctuation so that what's left are the more contentful words.  This is all an approximation, but the idea is that `.` and `the` and `and` are not providing useful information about the contents of a review, so we want to filter all those out.  The are, in NLP terminology, \"stopwords.\" And NLTK has a corpus of them (in several languages, we'll use the English ones).\n",
    "\n",
    "So what we are going to do is create a filtered version of these reviews, where the stopwords are removed and the remaining words are lowercased.\n",
    "\n",
    "The first step is to make a list of words we will include.  This list will include the stopwords as well as the \"punctuation words\" (a list of words, each of which is one punctuation mark)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rO3rxX1bpHQF",
    "outputId": "52b07b79-70bb-467a-f1bb-9b1e23ab1228"
   },
   "outputs": [],
   "source": [
    "# make the stopwords corpus available.\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "x35Xdvkm61pQ"
   },
   "outputs": [],
   "source": [
    "# make the punctuation list available (which is something that \"string\" knows how to do)\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jM7-UhXa2KMb"
   },
   "source": [
    "Now that the stopwords and punctuation list have been loaded up, we want to combine them into a list of words to exclude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GIfs7Hcx2Yy1",
    "outputId": "ab4b948c-9159-45ff-aea7-7d707625ce22"
   },
   "outputs": [],
   "source": [
    "# if we inquire about the fileids in the stopwords corpus,\n",
    "# we see the list of languages that it has stopwords for.\n",
    "print(stopwords.fileids())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "Hp7mCgBp2rMG"
   },
   "outputs": [],
   "source": [
    "# the ones we want are the English ones, so we use words() to pull those out.\n",
    "eng_stopwords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5B3irGNI20pz",
    "outputId": "6dea38a9-abaf-4a7f-fe5c-80ff41efdf5a"
   },
   "outputs": [],
   "source": [
    "# let's take a look at what we got.  It's a list of words.  They're all lowecase.\n",
    "print(eng_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QLlNzu763AaR"
   },
   "source": [
    "We now want to augment the list of stopwords with \"punctuation words\". Most tokenizers (which split raw text into words) and corpora split off punctuation into their own words, so a sentence \"I left.\" in a corpus might be represented like `['I', 'left', '.']`, where the `.` represents its own word.  We want to filter out those punctuation marks as well as the stopwords, so we're planning on making a bigger list of words that contains all the stopwords and the punctuation words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tpgoEUrV3faf",
    "outputId": "d2e061b5-15b4-4ae5-8acb-9069ed772c10"
   },
   "outputs": [],
   "source": [
    "# here is a string containing all the relevant punctuation marks.\n",
    "print(string.punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lGw18qxdeJHM"
   },
   "source": [
    "A string can be viewed as a list of characters (sort of, it isn't *actually* a list, but a lot of things you can do with lists you can also do with strings). Among the things you can do with a string is iterate through it, character by character, just like you would iterate through a regular list.\n",
    "\n",
    "Just to show this in action, consider the code below. It prints each punctuation character between square brackets.  The `end=''` part of the `print()` command tells Python not to move to the next line but just keep printing on the same line (if you do not specify the end parameter, the default is to move to the next line).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3iEyOpxU3mE0",
    "outputId": "02de1be8-50f7-4556-9cb5-bbb018f08e0a"
   },
   "outputs": [],
   "source": [
    "for c in string.punctuation:\n",
    "  print(' [{}]'.format(c), end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "jSgKg72f4lQa"
   },
   "source": [
    "### q2 (create punctuation word list) ###\n",
    "\n",
    "**Question**: Create a list of words formed from the punctuation string.  I expect you'll use a list comprehension. Though if you do, take a look at what the list comprehension is doing. There's actually an even easier way to get the same result, but however you get the result is fine.  Your list should look like `['!', '\"', '#', '$', etc.]` once you've formed it.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q2\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BkQVj9J4476M",
    "outputId": "a09e9834-ccb9-4678-f9f4-0476849720a7"
   },
   "outputs": [],
   "source": [
    "punc_words = ...\n",
    "print(punc_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VQkq58Yu5OF_"
   },
   "source": [
    "Now that we have `punc_words` and `eng_stopwords` defined, we can create a full list of words to filter out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qs0z6-Zh5SSd",
    "outputId": "15edda38-49a7-47f7-9fd6-7f83ee63907f"
   },
   "outputs": [],
   "source": [
    "skipwords = eng_stopwords + punc_words\n",
    "print(\"We have {} words to filter out.\".format(len(skipwords)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YfjCH7I_6bC2"
   },
   "source": [
    "Ok, so now we have the list of words we want to filter out, we can finally go back to address the reviews. Let's walk through the steps of lowercasing and filtering a single review by hand first, then we can generalize it to a function that we can apply to each of the reviews.  (One reason we want to lowercase the reviews is that the stopwords are all lowercase, so we want to filter out not only \"my\" in the middle of a sentence but also \"My\" at the beginning of one.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "iPGUEFTa7PfN"
   },
   "source": [
    "### q3 (fileid of first positive review) ###\n",
    "\n",
    "**Question:** \n",
    "Find the `fileid` of the first positive review.  You can do this however you like, just get to the right answer.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q3\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "XcgO_utAyOaj"
   },
   "outputs": [],
   "source": [
    "# Find the fileid of the first positive review\n",
    "# Do this however you want to, just get to the right answer.\n",
    "file_first_pos = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kQddUOD_Kf-g"
   },
   "source": [
    "Once we have the first positive review identified, we will retrieve the words from it, by asking the `movie_reviews` object to give us words for the `fileids` provided (in this case, just the one)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ywYFTzn5qFL1",
    "outputId": "8c47aa38-915e-4dc6-8dc5-8b3b16b3fac7"
   },
   "outputs": [],
   "source": [
    "# get the words for this review\n",
    "rev_words = movie_reviews.words(fileids=file_first_pos)\n",
    "# print out how many words it has, as a sanity check\n",
    "# to give us some confidence that rev_words indeed contains a review\n",
    "# It should have 862 words.\n",
    "print(\"The review has {} words\".format(len(rev_words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "EyLBPLpS0Zpo"
   },
   "source": [
    "### q4 (convert words to lowercase) ###\n",
    "\n",
    "**Question:** Now, convert all the words to lowercase, so that `rev_lower` is a list of the words in the first review, except with all of the words in lowercase.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q4\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "uldz9pkF00YD"
   },
   "outputs": [],
   "source": [
    "# now we want to convert all the words to lowercase\n",
    "rev_lower = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j-D06tDy7vV1"
   },
   "source": [
    "Now that we have our review in `rev_lower`, we can pull out any matching stopwords and punctuation.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "S90HayMw2fcK"
   },
   "source": [
    "### q5 (remove skipwords) ###\n",
    "\n",
    "**Question:** Define `rev_filtered` to be a list of the words from `rev_lower` that are not in `skipwords`.  You probably want to use a list comprehension for this.  You should wind up with 402 words after filtering.  The test is checking for that.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q5\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "khy1NAJxrCN_",
    "outputId": "1f2e4298-fa1e-4531-aff9-5e5be30a5d56"
   },
   "outputs": [],
   "source": [
    "# now we want to make a list of just those words in this review that\n",
    "# are NOT in skipwords.\n",
    "rev_filtered = ...\n",
    "# and print out how many words are left after removing the stopwords\n",
    "print(\"After filtering, the review has {} words.\".format(len(rev_filtered)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tzCsAZzHr9UF"
   },
   "source": [
    "Now that we've stepped through the procedure for a single review, let's generalize that so that we can do this for any review. We'll define a function `filter_review()` that goes through those same steps for any list of words (assumed to be a review) that is fed into it.  Specifically, it will make it lowercase, and then filter out the skipwords, and return the resulting list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "eFLsTSsGoSLP"
   },
   "outputs": [],
   "source": [
    "def filter_review(review_words, words_to_skip):\n",
    "  rev_lower = [w.lower() for w in review_words]\n",
    "  return [w for w in rev_lower if w not in words_to_skip]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kHhMAHmGt5HP",
    "outputId": "d16a9afb-01b3-459e-ea9b-f149cf8f6048"
   },
   "outputs": [],
   "source": [
    "# to see if it worked, let's try it on the review we did by hand.\n",
    "# just to make sure we get the same result\n",
    "rev_filtered = filter_review(rev_words, skipwords)\n",
    "num_filtered_words = len(rev_filtered)\n",
    "print(\"The review has {} non-stopwords.\".format(num_filtered_words))\n",
    "if num_filtered_words == 402:\n",
    "  print(\"\\o/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MV6ZAPkOGzQ6"
   },
   "source": [
    "A couple of notes here, particularly if Python is kind of new to you.  The `filter_review` function has two arguments, the first is a list of the words in the review, and the second is a list of words to filter out.  We have defined a list of words in the general context of this notebook that we want to filter out, we called it `skipwords`, and when we call the `filter_review` function, we will pass it that list.  It would have been possible to just refer to `skipwords` directly inside this function (that is, to have just assumed that it is already defined to be the right thing and use it), but that is not great programming practice. The reason is just that we usually want our functions to be as modular as they can be, so that we can reuse them.  Ideally, we want not to make assumptions about what people have defined in the outside context.  Better to define a function that takes as arguments all the information it needs to perform its function.\n",
    "\n",
    "This relates to \"variable scope\" if you wanted to look it up.  This mainly relates to what definitions are visible from where; if you define a variable in the \"global\" context of the notebook, functions can in principle see and use (and even change) those values.  If you define a variable within a function, it is only visible from elsewhere in that function, and not from outside. Maybe I'll demonstrate this a bit later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-jZ7clMfujb_"
   },
   "source": [
    "Ok, now we're ready to do what we just did for each of the reviews.  We'll start by doing it for the positive reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "c8ZrhTr1uhY9"
   },
   "outputs": [],
   "source": [
    "# Apply the filter function to all the positive reviews.\n",
    "pos_fileids = movie_reviews.fileids(categories='pos')\n",
    "pos_filtered = [filter_review(movie_reviews.words(f), skipwords) for f in pos_fileids]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rk3Ra0e_tdpR"
   },
   "source": [
    "Note that this actually took a little while.  It's going through 1000 reviews, and for each one retrieving the words, then going through them to make sure each one isn't in `skipwords`. This isn't even all that large a dataset, either. So it becomes important to be somewhat efficient in your operations if your datasets get larger. Or, have a lot of patience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H4jQQwI8vbtj",
    "outputId": "30206036-76b5-4801-c05c-fdcf18f7afb5"
   },
   "outputs": [],
   "source": [
    "# did it work? The first one should have 402 words in it.\n",
    "num_filtered_first_pos = len(pos_filtered[0])\n",
    "print(\"Filtered, first pos rev has {} words.\".format(num_filtered_first_pos))\n",
    "if num_filtered_first_pos != 402:\n",
    "  print(\"Disaster! Something is wrong. This number is not right.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IR9pV0sG73-H"
   },
   "source": [
    "Now we have processed all the positive reviews, so we should do the negative ones too. We could just repeat what we did above except for the negative ones. Let's do that quickly.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "Q8CdWlS8uVVj"
   },
   "source": [
    "### q6 (define neg_filtered) ###\n",
    "\n",
    "**Question:** Define `neg_fileids` and `neg_filtered` to provide a list of filtered reviews for the negative ones, just modeling your answer on what we did just above for the positive ones.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q6\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "FFwP8hFYuZxo"
   },
   "outputs": [],
   "source": [
    "neg_fileids = ...\n",
    "neg_filtered = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q6\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vq6WXmnXuh_-"
   },
   "source": [
    "Now we have two lists, one a list of negative reviews, one a list of positive reviews. Each one lowercased and filtered.\n",
    "\n",
    "The goal is to train a classifier on these, so that it can guess whether a review it sees is positive or negative. To train it, we need to give it a bunch of examples of positive ones and a bunch of examples of negative ones, with a label on each one that says what the right answer is so that, while it is training, it can check to see if it got it right and adjust itself if it didn't.\n",
    "\n",
    "The format these need to be in is essentially:\n",
    "\n",
    "```\n",
    "[(question, answer), (question, answer), ... ]\n",
    "```\n",
    "That is, a list of pairs, where each pair has a \"question\" (the thing being classified) and an \"answer\" (what the network should be classifying it as). In this case, the answer is the category from which the review was drawn from.\n",
    "\n",
    "A straightforward way to get these pairs is just build them with list comprehensions, as follows. Then add the pairs together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "iviM_JNKukUm"
   },
   "outputs": [],
   "source": [
    "pos_pairs = [(review, 'pos') for review in pos_filtered]\n",
    "neg_pairs = [(review, 'neg') for review in neg_filtered]\n",
    "all_pairs = pos_pairs + neg_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G0AZ817wwFdU",
    "outputId": "b200ebd8-6296-4684-c2e8-8abfeb775e03"
   },
   "outputs": [],
   "source": [
    "# see if we have something like what we expect\n",
    "# specifically, the first pair should hava:\n",
    "# a list of words in the first review as its first member, and\n",
    "# the category/answer (pos) as its second member\n",
    "first_pair = all_pairs[0]\n",
    "print(first_pair[0])\n",
    "print(first_pair[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4tJlH_zX4AK-"
   },
   "source": [
    "Great, now we have a big list of pairs in `all_pairs`.  Remember that each of those pairs is of the form `(words, category)` where `words` is a list of words in a given review, and `category` is either `pos` or `neg` depending on which category the review came from.\n",
    "\n",
    "The last step here is to create a `NaiveBayesClassifier` and train it. What a `NaiveBayesClassifier` does is looks at a set of properties of each review, and correlates the properties with the category we tell it the review had. The classifier can't read the review directly, so we have one more step to take before we can train a classifier. We have to decide what properties of the review the classifier will have access to. What is important?\n",
    "\n",
    "We'd already kind of decided that what we'll care about is what words are in it.  So the properties of the reviews are going to be something like `contains(terrible)` or `contains(exhilirating)`.  Then the classifier will look at those properties and learn to judge how likely a review is to be negative or positive based on the values of those properties.\n",
    "\n",
    "The first thing we could try is just to make a set of properties for each review that has `contains(w)` for every word `w` in the review.  We can use `set()` here because we are (by hypothesis/assumption) supposing that it doesn't matter how many times `terrible` appears in a review, only whether it appears or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "_rSO3bNH6RoB"
   },
   "outputs": [],
   "source": [
    "def extract_features(words):\n",
    "  features = {}\n",
    "  for w in set(words):\n",
    "    features['contains({})'.format(w)] = True\n",
    "  return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TYdZeS8cEmvL"
   },
   "source": [
    "Let's try this on one review. It'll be the first one. The first member of the list `all_pairs` is a pair. That pair has as its first member the filtered review (and as its second member the category). So, the review is in `all_pairs[0][0]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iRYKlrzx6mTq",
    "outputId": "83baad25-18a8-471f-df37-d67555c727a8"
   },
   "outputs": [],
   "source": [
    "print(extract_features(all_pairs[0][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sd-pL1omEbeo"
   },
   "source": [
    "We've just done this for one review (the one at the beginning of `all_pairs`), now let's apply that to all the reviews.  We're converting a pair like `(review, category)` to a pair like `(features, category)`. That is, we need to retain the cateogry as the second member of the pair, but extract the features from the review and use those features as the first member of the pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "XcTxZKtB8-uu"
   },
   "outputs": [],
   "source": [
    "# Now that we've done that for one, let's do it for all of them.\n",
    "feature_pairs = [(extract_features(words), cat) for (words, cat) in all_pairs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "426riTJn9QIi",
    "outputId": "dd22cc7f-a7e7-4726-85a6-cdf898544296"
   },
   "outputs": [],
   "source": [
    "# double check to see if this is what we expected to get\n",
    "print(feature_pairs[0][0])\n",
    "print(feature_pairs[0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ojn19vza9x88"
   },
   "source": [
    "## Training the classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lFNeqn439t9v"
   },
   "source": [
    "Now, let's try to make a `NaiveBayesClassifier`. Ths first thing to do is to shuffle the `feature_pairs` list (so that all the negative reviews aren't at the front), split it into a training set and test set, train a `NaiveBayesClassifier` on the training set, and see how it does on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "chqEc8p730s3"
   },
   "outputs": [],
   "source": [
    "# make Python aware of the randomization commands\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "mrgpg8DR4lnU"
   },
   "outputs": [],
   "source": [
    "# now shuffle feature_pairs\n",
    "random.shuffle(feature_pairs)\n",
    "# they should now be all jumbled up\n",
    "# let's take the first 600 (about 30%) as the test set, the rest as the training set\n",
    "test_rev = feature_pairs[:600]\n",
    "train_rev = feature_pairs[600:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "uGd8o2iJ44Pa"
   },
   "outputs": [],
   "source": [
    "# ask nltk to create and train a classifier\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_rev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3jwY6i62-IZL",
    "outputId": "b1baa65a-e92b-49d2-9493-e7ae9873b561"
   },
   "outputs": [],
   "source": [
    "# ok, now it is all trained up, how did it do?\n",
    "print(nltk.classify.accuracy(classifier, test_rev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AMqS8WsM-PjZ",
    "outputId": "f13bd403-c255-486d-f20d-bb3a20e1d4ca"
   },
   "outputs": [],
   "source": [
    "# compare that to how well it learned its training set\n",
    "print(nltk.classify.accuracy(classifier, train_rev))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9HnHHvEe-bEd"
   },
   "source": [
    "The classifier is doing better than chance (better than guessing) at identifying positive and negative reviews for reviews it hasn't seen before, but there's a **BIG** difference between how well it does on its training data and how well it does on its test data. That's not optimal, it kind of looks like it memorized the training data. That's called \"overfitting\" and basically means that it's not likely to generalize well if it is memorizing idiosyncracies of the training set.\n",
    "\n",
    "Because this data set is actually kind of small, this is probably not the right place to talk about the issues with overfitting, because there's not much improvement we can squeeze out of this on the test set by avoiding overfitting.  But one thing to consider is, how accurate *should* it be if given just a list of the words contained in a review at guessing whether it's positive or negative? It's very suspicious if it is 98% accurate; no person should be able to do that (given an unordered list of words in a review, guess to 98% accuracy whether it is positive or negative).  Anyway, we'll come back to overfitting later.  For the moment, we can at least be happy that it is getting around 70% on reviews it has never seen before.\n",
    "\n",
    "We can get some insight into how it is making its decisions by asking the classifier what the most informative features are.  You'll see something slightly different from everyone else, given that it is based on a random shuffling, but it will tell you something like: a review that contains the word \"outstanding\" is about 20-to-1 odds of being a positive review, and one that contains \"turkey\" is about 18-to-1 odds of being a negative review.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eKzz8iqBLzDC",
    "outputId": "4961ac7b-8011-41c5-b18a-467124e5cb2b"
   },
   "outputs": [],
   "source": [
    "classifier.show_most_informative_features()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dw3eAkKNQzDi"
   },
   "source": [
    "# Sentiment analysis on Twitter samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nu294lsrTjrI"
   },
   "source": [
    "Let's do something else similar that somehow seems more relevant.  NLTK has a set of Twitter samples that we can use.  We did/will kind of run through some of this in class, but watching me do it is not quite the same as doing it yourself, so let's try this out in the context of a homework assignment.\n",
    "\n",
    "> This exercise is largely modeled on [Shamuik Daityari's tutorial](https://www.digitalocean.com/community/tutorials/how-to-perform-sentiment-analysis-in-python-3-using-the-natural-language-toolkit-nltk)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D8x9GD_D97xx"
   },
   "source": [
    "## Preparing the corpus data\n",
    "\n",
    "First, we make the corpus available.  Again, involving the two steps of `from nltk.corpus import` and `nltk.download`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Tvi3RD12ehQe",
    "outputId": "422d3a5e-1b3a-4129-ceef-e04ef0e4938a"
   },
   "outputs": [],
   "source": [
    "# make the twitter samples corpus available\n",
    "from nltk.corpus import twitter_samples\n",
    "nltk.download('twitter_samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NF1DnDxyeru0",
    "outputId": "6f2db0f7-3452-4339-df05-73d57a9cb8d5"
   },
   "outputs": [],
   "source": [
    "# what files are in this corpus?\n",
    "twitter_samples.fileids()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A6oKsNsET9HP"
   },
   "source": [
    "The `twitter_samples` corpus contains tokenized tweets (so, already broken up into words), and we can just use those. If we request `tokenized(fileid)` of the corpus object, we will get those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Go_kJo1XUBjq",
    "outputId": "d6b63ac2-0f08-4593-97b2-246ff73ba3b9"
   },
   "outputs": [],
   "source": [
    "# look at the first tokenized positive tweet (a list of words)\n",
    "first_tweet = twitter_samples.tokenized('positive_tweets.json')[0]\n",
    "print(first_tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-xl8o09jdlOg"
   },
   "source": [
    "We're going to do a couple of things that are a little bit more sophisticated with these. The first thing is to do something analogous to what we did when we lowercased all the words. The reason we lowercased the words before is so that we aren't treating \"Happily\" at the beginning of a sentence different from \"happily\" in the middle of the sentence, etc. What we're going to do here is one further level of abstraction by attempting to neutralize the difference between tense and agreement realizations.  That is, we want to make \"annoy\", \"annoying\", \"annoyed\" all come out as the same concept/word.\n",
    "\n",
    "The process of removing inflection like this is called \"stemming\" and there are various ways that it can be done, but we'll just use a built-in way to do this using NLTK.  We're going to use the WordNetLemmatizer, which can strip the endings off of verbs and nouns if you tell it what kind of word it is.  (A \"lemma\" is basically the same as a \"stem\", so this stemmer is called a lemmatizer, because, I don't know why.) \n",
    "\n",
    "Let's load that up.\n",
    "\n",
    "> The \"omw\" is the Open Multilingual WordNet.  This is a relatively recent addition to NLTK, but is now required to be able to use the stemmer/lemmatizer. The instruction used to be to download 'punkt' but 'omw-1.4' now replaces that.  This might be useful to know if you look at examples out on the web that are older than a few months.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pXk_ZzfTp_Af",
    "outputId": "0b07dd72-b2eb-403c-dd05-0cec54b1296c"
   },
   "outputs": [],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KWHiDYQUQpE-"
   },
   "source": [
    "Here is what the stemmer/lemmatizer will do: if you give it a singular noun and a plural noun, it should return some neutral form that doesn't distinguish them.  If you give it a past tense verb and a present tense verb, it should give you some neutral form. But you do have to tell it whether you are trying to \"stem\" verbs or nouns.  That matters to its algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kcReQfliQ_Tg",
    "outputId": "45748e48-eb00-4495-8237-e87116a12e35"
   },
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "print(lemmatizer.lemmatize('is', 'v'))\n",
    "print(lemmatizer.lemmatize('being', 'v'))\n",
    "print(lemmatizer.lemmatize('cried', 'v'))\n",
    "print(lemmatizer.lemmatize('crying', 'v'))\n",
    "print(lemmatizer.lemmatize('taken', 'v'))\n",
    "print(lemmatizer.lemmatize('took', 'v'))\n",
    "print(lemmatizer.lemmatize('sing', 'v'))\n",
    "print(lemmatizer.lemmatize('singing', 'v'))\n",
    "print(lemmatizer.lemmatize('stapler', 'n'))\n",
    "print(lemmatizer.lemmatize('staplers', 'n'))\n",
    "print(lemmatizer.lemmatize('ox', 'n'))\n",
    "print(lemmatizer.lemmatize('oxen', 'n'))\n",
    "print(lemmatizer.lemmatize('person', 'n'))\n",
    "print(lemmatizer.lemmatize('people', 'n'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3LAbBZPbRoV8"
   },
   "source": [
    "Pretty good. It didn't manage to quite factor out plurality for \"people\" but it did pretty well anyway.\n",
    "\n",
    "Now: what we want to do is stem the words in our tweets.\n",
    "\n",
    "Except, wait.  We need to know what the nouns and verbs are.  And we don't have that information.  So first we need to classify the words in the tweets as nouns or verbs.\n",
    "\n",
    "That is in fact the same kind of problem we're already in the midst of trying to solve, it classifies words by part of speech.  However, we'll just use a pre-existing classifier for that.  NLTK contains a pretrained network that can look at words and guess their part of speech, that will be good enough for this.  So, let's load up this \"tagger\" (named that way because it tags words with their parts of speech).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hgI-PqHgqJpW",
    "outputId": "b84c1f50-b861-4e9a-d326-28ceb949b1d0"
   },
   "outputs": [],
   "source": [
    "from nltk.tag import pos_tag\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SNcjZYjGfzRK"
   },
   "source": [
    "And then let's see what it does.  Let's apply it to that first tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EU25DxbDf8MI",
    "outputId": "f4dcd5b9-ad22-47e5-919b-ede04f24c5e2"
   },
   "outputs": [],
   "source": [
    "print(pos_tag(first_tweet))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oaCiUa0MgOI9"
   },
   "source": [
    "So it has converted our list of words into a list of pairs. The first member of the pair is the word, the second member of the pair is the tag (that is, the part of speech). We learn from this that it considers \"#FollowFriday\" to be an adjective.  I guess.\n",
    "\n",
    "In there, we have a verb \"being\" and a noun \"members\".  We know that \"being\" is a verb because it has been given a part-of-speech tag of \"VBG\", and that \"members\" is a noun because it has been given a part-of-speech tag of \"NNS\".  Specifically, those first two letters of the part-of-speech tag are what tells us if it is a noun, if it is a verb, etc.  And the lemmatizer only cares about nouns and verbs (none of the more specific information), so we just give it \"n\" or \"v\" as the part of speech.\n",
    "\n",
    "Let's try this out by hand on those two words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GKMmumTdgj_l",
    "outputId": "b5b8537f-5c2b-4647-f88c-8dac973573a7"
   },
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "print(lemmatizer.lemmatize('being', 'v'))\n",
    "print(lemmatizer.lemmatize('members', 'n'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qxX7S-tOhg4L"
   },
   "source": [
    "Now we'll define something that will take a whole sentence, figure out the part of speech of each word, and -- if the word is a noun or a verb -- stem it.  We'll do other processing with it as well, while we're at it.  Specifically, we'll lowercase all the words, and we'll exclude all the skipwords as well (stopwords for English and punctuation), and drop any @-mentions and links from consideration.\n",
    "\n",
    "> You will see `continue` in the code below.  What this does is stops processing of the current block and returns control to the `for...:` to move on to the next word (if any are left).  So, if `continue` is processed before we add the token to the `cleaned_tokens` list, the token does not get added, we just move on to consider the next token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "jPBPa2iju00C"
   },
   "outputs": [],
   "source": [
    "def clean_tweet(tweet_tokens, skipwords=()):\n",
    "  # we will return a list of \"cleaned\" tokens, which excludes\n",
    "  # all of the stopwords, punctuation, @-mentions, and\n",
    "  # links, and has all the tokens lowercased and stemmed.\n",
    "  # the cleaned_tokens list is where we are collecting those.\n",
    "  # so we start off by making it an empty list, which we will\n",
    "  # then add to as we go through the tweet\n",
    "  cleaned_tokens = []\n",
    "  # define the lemmatizer\n",
    "  lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "  # pos_tag() will convert the tweet into a list of pairs\n",
    "  # each pair will have the word as its first member\n",
    "  # and the part of speech as its second\n",
    "  for token, tag in pos_tag(tweet_tokens):\n",
    "\n",
    "    lowercased_token = token.lower()\n",
    "\n",
    "    # if this word is one of the skipwords (stopwords + punctuation)\n",
    "    # then skip it\n",
    "    if lowercased_token in skipwords:\n",
    "      continue\n",
    "    # if this is an @-mention or a web address, then skip it\n",
    "    if lowercased_token.startswith('http') or lowercased_token.startswith('@'):\n",
    "      continue\n",
    "\n",
    "    # set part of speech for this word to be n or v if the pos tagger\n",
    "    # said it was a noun or a verb, otherwise set the part of speech to be a.\n",
    "    if tag.startswith('NN'):\n",
    "      pos = 'n'\n",
    "    elif tag.startswith('VB'):\n",
    "      pos = 'v'\n",
    "    else:\n",
    "      pos = 'a'\n",
    "    \n",
    "    # stem the word\n",
    "    token = lemmatizer.lemmatize(lowercased_token, pos)\n",
    "    # add the stemmed word to the list\n",
    "    cleaned_tokens.append(token)\n",
    "  return cleaned_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HZOUsW7MlNAZ"
   },
   "source": [
    "Read through the function above to see what it is doing.\n",
    "\n",
    "Let's see what it did to the first tweet, by printing the un-cleaned tweet for comparison, and then the cleaned tweet.\n",
    "\n",
    "(Remember that we defined `skipwords` a long time ago, but it contains the English stopwords and the punctuation words.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PUVR6-SpiyJm",
    "outputId": "766906c5-29f7-4740-bf4b-2c743142c55c"
   },
   "outputs": [],
   "source": [
    "# skipwords = stopwords.words('english') + list(string.punctuation)\n",
    "print(\" \".join(first_tweet))\n",
    "print(\" \".join(clean_tweet(first_tweet, skipwords)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rMWYx23amiqz"
   },
   "source": [
    "Ok, now that we've worked through the first tweet by hand, let's prepare to process all the tweets this way.\n",
    "\n",
    "First, we'll collect all the tokenized tweets into named variables so we can work with them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "bW1ykNPufAvt"
   },
   "outputs": [],
   "source": [
    "# collect all the tokenized tweets\n",
    "pos_tweets = twitter_samples.tokenized('positive_tweets.json')\n",
    "neg_tweets = twitter_samples.tokenized('negative_tweets.json')\n",
    "test_tweets = twitter_samples.tokenized('tweets.20150430-223406.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xvWK7ec08G-_"
   },
   "source": [
    "Finally, something for you to do again, after all that reading.  We have a list of positive tweets in `pos_tweets` and we now want to clean them all using `clean_tweet()` to get a list of cleaned positive tweets.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "Jtr871nwmqVB"
   },
   "source": [
    "### q7 (clean positive tweets into pos_cleaned) ###\n",
    "\n",
    "**Question:** Define a list called `pos_cleaned` that is a list of the results of calling `clean_tweet()` for each tweet in `pos_tweets`.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q7\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "MSUlAaFVxADh"
   },
   "outputs": [],
   "source": [
    "pos_cleaned = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q7\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "wBr3fSHcnnJm"
   },
   "source": [
    "### q8 (clean negative tweets into neg_cleaned) ###\n",
    "\n",
    "**Question:** Now do the analogous thing to create `neg_cleaned` that is a list of the results of calling `clean_tweet()` for each tweet in `neg_tweets`.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q8\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "tR9OL5E0nwvE"
   },
   "outputs": [],
   "source": [
    "neg_cleaned = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1CoP4U2YpIo8"
   },
   "source": [
    "Just to see what we've got, let's look at the 501st positive tweet in its original form and then in its cleaned version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7QlliaAsxMPr",
    "outputId": "cdcf01c3-4c31-4e81-8c9b-c795051f02bd"
   },
   "outputs": [],
   "source": [
    "print(pos_tweets[500])\n",
    "print(pos_cleaned[500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sQ0xZNPYpUSs"
   },
   "source": [
    "Now, it is time for a hypothesis.  We have the cleaned data and we need to figure out what information we think our classifier is going to need in order to make the call on whether a tweet is positive or negative.\n",
    "\n",
    "For the moment, we're going to use a simple hypothesis.  It'll be like the movie reviews, we'll suppose that there are words that are common in positive tweets and not in negative tweets, and words that are common in negative tweets and not in positive tweets.\n",
    "\n",
    "We can actually just use the `extract_features()` function defined a while ago, which just adds a feature for each word in the tweet.  As a reminder, that was defined like this:\n",
    "\n",
    "```python\n",
    "def extract_features(words):\n",
    "  features = {}\n",
    "  for w in set(words):\n",
    "    features['contains({})'.format(w)] = True\n",
    "  return features\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "id": "7aO--b0-kIcL"
   },
   "outputs": [],
   "source": [
    "# create the list of pairs\n",
    "pos_tagged_tweets = [(extract_features(t), 'pos') for t in pos_cleaned]\n",
    "neg_tagged_tweets = [(extract_features(t), 'neg') for t in neg_cleaned]\n",
    "tagged_tweets = pos_tagged_tweets + neg_tagged_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6a0rWi0EkhQs",
    "outputId": "0b13db08-cd29-4b68-80ba-ce82f8d59168"
   },
   "outputs": [],
   "source": [
    "# make sure they look like we're expecting them to\n",
    "tagged_tweets[500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YI5i1B3p-N_I"
   },
   "source": [
    "## Training the classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sDgn7AYQ8ZP5"
   },
   "source": [
    "Now that we have all the tagged tweets, we just need to jumble them up, split them into a training set and a test set, train a `NaiveBayesClassifier` on the training set and test it on the testing set.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "1EtnP_NYk6uu"
   },
   "source": [
    "### q9 (train and test a NaiveBayesClassifier) ###\n",
    "\n",
    "**Question:** So, do that.  Figure out how many tweets there are in `tagged_tweets`, shuffle them, cut off the first 30% as the test set, leaving the last 70% as the training set, train a new `NaiveBayesClassifier` on the training set and see how well it does on the test set. I've provided a couple of prompts (partly so that I can control what variable names you use, so that the results can be checked).\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q9\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "quEt6LFtmuDH",
    "outputId": "84d19472-f5ac-4030-d816-dce0f91c9d02"
   },
   "outputs": [],
   "source": [
    "# shuffle tagged_tweets\n",
    "...\n",
    "# determine the number of tweets in tagged_tweets\n",
    "num_tweets = ...\n",
    "print(\"A test set with 30% in it will have {:d} tweets\".format(int(.3*num_tweets)))\n",
    "# define test_tweets to be the first 30%, train_tweets to be the rest\n",
    "test_tweets = ...\n",
    "train_tweets = ...\n",
    "# define and train a new NaiveBayesClassifier on train_tweets\n",
    "tweet_classifier = ...\n",
    "# test and print out how successful the classifier is on test_tweets\n",
    "accuracy = ...\n",
    "print(\"The classifier got an accuracy score of {}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q9\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HuGuvIvksCiJ"
   },
   "source": [
    "That's really, really good.\n",
    "\n",
    "Really very good.\n",
    "\n",
    "That's kind of surprisingly good. What did it use to make these calls?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "59zOqWuMohS4",
    "outputId": "8fa88ba2-71e7-4475-f04d-6ef8c1d7a650"
   },
   "outputs": [],
   "source": [
    "tweet_classifier.show_most_informative_features()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G6gCNzGbsga-"
   },
   "source": [
    "Well, now, wait a minute. It seems like a great many of these tweets have basically an \"I am a positive tweet\" or \"I am a negative tweet\" right in them.  How many had these?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FVAOPGB3sXcL",
    "outputId": "c80af953-f3c8-4122-ca6c-a7832d12c68a"
   },
   "outputs": [],
   "source": [
    "pos_smileys = sum([':)' in tw or ':-)' in tw for tw in pos_cleaned])\n",
    "pos_frownies = sum([':(' in tw or ':-(' in tw for tw in pos_cleaned])\n",
    "neg_smileys = sum([':)' in tw or ':-)' in tw for tw in neg_cleaned])\n",
    "neg_frownies = sum([':(' in tw or ':-(' in tw for tw in neg_cleaned])\n",
    "print(\"positive tweets with smiley: {}%\".format(100*pos_smileys/len(pos_cleaned)))\n",
    "print(\"negative tweets with smiley: {}%\".format(100*neg_smileys/len(neg_cleaned)))\n",
    "print(\"positive tweets with frowny: {}%\".format(100*pos_frownies/len(pos_cleaned)))\n",
    "print(\"negative tweets with frowny: {}%\".format(100*neg_frownies/len(neg_cleaned)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xOByTo2-uELl"
   },
   "source": [
    "At the outset, this problem seemed like it was going to be a bit more challenging than it has turned out to be.\n",
    "\n",
    "But counting on a smiley/frowny seems a little bit like cheating.  What if we excluded those from consideration, and asked something like: what face would be included in this tweet if a face were to be included? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gLs17EymsNDG",
    "outputId": "85e7e9ba-c1e3-4a9e-d817-65b29dabb859"
   },
   "outputs": [],
   "source": [
    "skipfaces2 = skipwords + [':)', ':-)', ':(', ':-(']\n",
    "pos_cleaned2 = [clean_tweet(t, skipfaces2) for t in pos_tweets]\n",
    "neg_cleaned2 = [clean_tweet(t, skipfaces2) for t in neg_tweets]\n",
    "pos_tagged_tweets2 = [(extract_features(t), 'pos') for t in pos_cleaned2]\n",
    "neg_tagged_tweets2 = [(extract_features(t), 'neg') for t in neg_cleaned2]\n",
    "tagged_tweets2 = pos_tagged_tweets2 + neg_tagged_tweets2\n",
    "random.shuffle(tagged_tweets2)\n",
    "test_tweets2 = tagged_tweets2[:3000]\n",
    "train_tweets2 = tagged_tweets2[3000:]\n",
    "tweet_classifier2 = nltk.NaiveBayesClassifier.train(train_tweets2)\n",
    "print(nltk.classify.accuracy(tweet_classifier2, test_tweets2))\n",
    "tweet_classifier2.show_most_informative_features()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WZ5tn9C0mIFk"
   },
   "source": [
    "That's a bit more like it. It's still pretty good, it's right about three quarters of the time, even without smiley and frowny faces to guide it.\n",
    "(And if we explored this further, we may find that there are other emoticons we should be removing too, to keep from \"cheating\" this way.)\n",
    "\n",
    "Let's try making up a tweet just to see what it would do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C0k8SlGLitH8",
    "outputId": "7ae2c944-3c7b-4213-8f6c-9e07aa6eb263"
   },
   "outputs": [],
   "source": [
    "def test_sentence(sent):\n",
    "  words = sent.split()\n",
    "  clean_words = clean_tweet(words, skipfaces2)\n",
    "  result = tweet_classifier2.classify(extract_features(clean_words))\n",
    "  print(\"{}: {}\".format(result, sent))\n",
    "\n",
    "test_sentence(\"I just had the best salad of my life\")\n",
    "test_sentence(\"That salad was the worst thing I have ever eaten\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WgSCgqeLyYwJ"
   },
   "source": [
    "We can also ask our classifier what the actual numerical probability is (rather than just making the binary call). It will tell us \"pos\" if it is over 50% likely that it is positive, but it might me interesting to see the confidence it has.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Gt0xefHS01ss",
    "outputId": "428977cc-ce64-4e65-adfa-641ed177cdb4"
   },
   "outputs": [],
   "source": [
    "def test_sentence_prob(sent):\n",
    "  words = sent.split()\n",
    "  clean_words = clean_tweet(words, skipfaces2)\n",
    "  result = tweet_classifier2.prob_classify(extract_features(clean_words))\n",
    "  call = result.max()\n",
    "  odds = result.prob(call)\n",
    "  print(\"{} ({:6.2f}%): {}\".format(call, 100 * odds, sent))\n",
    "\n",
    "test_sentence_prob(\"I just had the best salad of my life\")\n",
    "test_sentence_prob(\"The salad was marginally better than anticipated\")\n",
    "test_sentence_prob(\"That salad was the worst thing I have ever eaten\")\n",
    "test_sentence_prob(\"That salad could have been better\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MUSq7EHT3cGS"
   },
   "source": [
    "It would seem that the classifier could be better at handling complex pragmatics. But, thinking that \"That salad could have been better\" is positive is a completely understandable error, since \"better\" is probably a pretty good signal of a positive tweet. \n",
    "\n",
    "We can look and see what contributions the individual words are making by evaluating a \"tweet\" that contains just that word.  And indeed, \"better\" pushes positive.  In fact, even \"salad\" does. Only \"could\" is pushing the other way.  So we can sympathize with our robot for thinking that \"That salad could have been better\" is a positive statement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SKQVY8o-4y3y",
    "outputId": "17687b6a-5724-4e9e-9956-5f612c44837f"
   },
   "outputs": [],
   "source": [
    "test_sentence_prob(\"that\")\n",
    "test_sentence_prob(\"salad\")\n",
    "test_sentence_prob(\"could\")\n",
    "test_sentence_prob(\"have\")\n",
    "test_sentence_prob(\"been\")\n",
    "test_sentence_prob(\"better\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "cRL5BRql8FEo"
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### q10 (thoughts) ###\n",
    "\n",
    "**Question:** What do you make of the result above for \"that\", \"have\", and \"been\"?  They're all the same, why are they all the same?  And, what might be the reason for them not being exactly 50%, assuming they aren't? (For me, I am seeing pos 50.67%, but there's a random element that should lead you to see something different.)  There is kind of a right answer here, and it's not just that \"that\" is kind of a grammatical word that doesn't add much to the meaning, although that is in a sense related to the answer.  There's something more concrete at play here that leads them all to have the same value, and leads that value to be not quite exactly 50%.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q10\n",
    "manual: true\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7EAMOIWa5wuO"
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "\n",
    "\n",
    "At this point I was intending to add in something about authorship attribution, but it's a pretty similar endeavor; instead of classifying something as positive or negative, you are classifying something as one author or another.  So, this is long enough, we'll leave homework 2 at that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cqsviImFiIu3"
   },
   "source": [
    "# Chat corpus, dialogue acts (advanced, optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sj4tpTi-rZwZ"
   },
   "source": [
    "Let's use the [NPS Chat Corpus](https://faculty.nps.edu/cmartell/NPSChat.htm). These are things taken from a few chat rooms in 2006, and they are tagged both for part of speech and for dialogue-acts.  Dialogue-acts are like \"ynQuestion\", \"Statement\", and some other things.  There is a description and list at the link above, and [a bit more detailed description at the Linguistics Data Consortium](https://catalog.ldc.upenn.edu/LDC2010T05).\n",
    "\n",
    "Download and import.  We will abbreviate `nps_chat` as `nps` for typing convenience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A8Sc6aTPrUJR",
    "outputId": "3dab6c03-ec77-460c-d2ca-fcf408b9cd4d"
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import nps_chat as nps\n",
    "nltk.download('nps_chat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lQm9AnLmr1xb",
    "outputId": "cb356dd5-3fde-472c-a2ee-0e478c5e952d"
   },
   "outputs": [],
   "source": [
    "# Here are the files.  The filename indicates date, age range, number of posts\n",
    "nps.fileids()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GGtKTPAi9cs2"
   },
   "source": [
    "My recollection of this corpus is that some of the text is kind of unpleasant.  These are chat rooms, this was 2006.  I'll pick one for an example.\n",
    "\n",
    "First, we retrieve the posts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "id": "BVDj9Xj9r7Dk"
   },
   "outputs": [],
   "source": [
    "posts = nps.xml_posts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YN7YCo_m-Ajo"
   },
   "source": [
    "Now, we'll retrieve one particular post.  Number 119.  The dialogue-act can be retrieved by using `p.get('class')`, and the user who typed it can be retrieved by using `p.get('user')`.  The user name is anonymized, but the number at the end should consistently identify the same speaker throughout the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hkrC_zG4r-dU",
    "outputId": "554d702c-911b-4e8e-9ffb-e26201daf35f"
   },
   "outputs": [],
   "source": [
    "p = posts[119]\n",
    "print(p.text)\n",
    "print(p.get('class'))\n",
    "print(p.get('user'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KzFl2q7b-hic"
   },
   "source": [
    "**Task.** Extract the complete list of dialogue acts represented in the list of posts.  The NPS page gives a list against which you can check.  Collect them and display them.  Count them.  There should be 15. Display them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sPrJJUM6_Qxq",
    "outputId": "faecb336-3d54-490c-9047-777fc43f8a35"
   },
   "outputs": [],
   "source": [
    "# retrieve the list of dialogue-acts\n",
    "acts = {'Statement', 'Bye'} # in this format\n",
    "print(\"If all is well, this should say 15: {}\".format(len(acts)))\n",
    "acts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1rUB9EIoABim"
   },
   "source": [
    "We will use a tokenizer (breaks up text into words) and this depends on `punkt`, so download that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xZxSgu7U1h7v",
    "outputId": "a934d2cc-46e2-4ba2-be7f-0701ef473fc1"
   },
   "outputs": [],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W95giS1LAK46"
   },
   "source": [
    "Here is a basic feature-extractor that we can use to begin with. Just making a feature out of each word in the chat sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "id": "nwVaP3hKsLYi"
   },
   "outputs": [],
   "source": [
    "def nps_features(post):  #v1\n",
    "  features = {}\n",
    "  words = nltk.word_tokenize(post.text)\n",
    "  for w in words:\n",
    "    features['contains({})'.format(w.lower())] = True\n",
    "  return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xfgjn8YXAoVG"
   },
   "source": [
    "Applied to post 119, it looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Kh-F0pl41aLA",
    "outputId": "23304728-7417-4169-c71e-af3f01019c88"
   },
   "outputs": [],
   "source": [
    "print(nps_features(posts[119]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b17I8FDAAvgR"
   },
   "source": [
    "**Task**. Now, make a list of pairs out of the posts. The first member will be the features provided by `nps_features` and the second member will be the classification (dialogue-act)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EcxlIIblA6JV",
    "outputId": "9b135a02-8900-4971-cc25-8511a1f61667"
   },
   "outputs": [],
   "source": [
    "# like this but with all of the posts in the list.\n",
    "fposts = [(nps_features(posts[119]), posts[119].get('class'))]\n",
    "print(\"There should be 10567 posts and there are {}\".format(len(fposts)))\n",
    "fposts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9OhBa2OQBV3N"
   },
   "source": [
    "**Task**. Split `fposts` into `nps_train` and `nps_test`, start by training on 80% and testing on 20%.  This should probably be randomized too, so that the test set doesn't come entirely out of one chat room/age group (on which the classifier was not trained)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "id": "DDbdJqz7DK9d"
   },
   "outputs": [],
   "source": [
    "# define nps_train, nps_test but make them 80%, 20% of fposts\n",
    "# also shuffle them first\n",
    "nps_train, nps_test = fposts[:1], fposts[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "31nh_ucL1894"
   },
   "source": [
    "Now, we will train the classifier and see how it did."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7MLf1cLL2FNg",
    "outputId": "fee8e5f2-fe3b-4e58-8032-28248225c37b"
   },
   "outputs": [],
   "source": [
    "nps_classifier = nltk.NaiveBayesClassifier.train(nps_train)\n",
    "print(nltk.classify.accuracy(nps_classifier, nps_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AEPqbuZZE81-"
   },
   "source": [
    "Not great.  The rest of this is just playing around trying to see what might improve this.\n",
    "\n",
    "This is a chat room.  It's possible at least that a `ynQuestion` might be more likely to be followed by a `yanswer` than by a `greeting`.  Perhaps we could use the context to make better predictions.  That is, might the category of the preceding line, as well as the words contained in the current line, be able to predict the category of the current line better?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dyd2AP0RF8K8"
   },
   "source": [
    "**Task**.  Try adding `prev-class` to the features of the posts that we train on.  (That is, in addition to all the `contains(like): True` type features we have, add something like `prev-class: ynQuestion` as well.  This probably can no longer be done with a list comprehension, I defined `fpost_list(posts)` that would return the list of (features, category) pairs.  That way it can go through the (unshuffled) posts, keep track of what the category of the previous text was, and record that in the features of the present chat text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "id": "_6i8B4s32J1Y"
   },
   "outputs": [],
   "source": [
    "# define something to give you a revised context-aware version of fposts\n",
    "# that records the class of the preceding line as one of the features\n",
    "# then randomize, split, train, test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xqRtyA_H4mka"
   },
   "source": [
    "Was it better?  For me, no.  For me, it was actually worse!\n",
    "\n",
    "**Task.** Continue to play with it.  Can you get it to do better?  Some ideas:\n",
    " - Maybe we should ignore messages in the \"System\" category?\n",
    " - Maybe try training and testing on subcorpora for just chat rooms with the same age rangs?\n",
    " - Maybe include the previous *two* message categories? Ordered or unordered?\n",
    "\n",
    "See what you can do.  For me, the best I ever got it to do is actually to include the *next* category as a feature.  That is, I built the set in reverse, so that the features of a message were the words it included and the category of the following message.  Perhaps a story can be told about how `yanswer` predicts a preceding `ynQuestion` effectively or something.  Curious to see if anyone can get it much over 70%, that was about as much as I could squeeze out of it.\n",
    "\n",
    "I suppose it's possible that a human looking at these wouldn't score super-high, except a human did the original classification.  However, perhaps not all humans would agree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KPW9A-PEXv7B"
   },
   "source": [
    "# Submission instructions ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IqO43UrDXzyJ"
   },
   "source": [
    "Go to File at the upper left of this web page and click \"Download .ipynb\" to download a copy of this.  Then go to Gradescope to submit the homework, and drag the .ipynb file in.  That should be all you need to do.  The autograder will run, but if your tests all passed in here, they should pass there as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rOMSpwT8YK3s"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
