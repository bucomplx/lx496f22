{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Tyatjjy5i1W"
      },
      "source": [
        "### LX 496/796  Introduction -- Parsing and context free grammars\n",
        "\n",
        "### Due Monday at 11:59 PM in Gradescope\n",
        "\n",
        "In this homework, you will become familiar with creating context free grammars in NLTK.  Also, parsing, drawing, and traversing syntactic trees.\n",
        "\n",
        "I'm not going to bother with the autograder, it wasn't behaving last week anyway.  I'll give you tests to check your work instead."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TsU52XEo5i1Z"
      },
      "source": [
        "# Getting started with a context-free grammar\n",
        "\n",
        "We'll now use NLTK to do a little bit of actual theoretical linguistics.\n",
        "This is at least partly based on chapter 8 of the NLTK book.\n",
        "\n",
        "As a first step, we're going to create a context-free grammar to play with.\n",
        "\n",
        "First, the standard `import nltk` part."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tooqWzLq5i1Z"
      },
      "outputs": [],
      "source": [
        "import nltk"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Second, install `svgling`. This is a package written by Kyle Rawlins (JHU) designed to allow NLTK to draw trees properly within Colab/Jupyter notebooks (among other things). It lives here: https://github.com/rawlins/svgling .\n",
        "\n",
        "If you don't install this, NLTK would by default want to open a new window to draw trees in.  That's not something Colab can do, and so you get an error.  Since we like trees better than we like errors, we will install this."
      ],
      "metadata": {
        "id": "ttue4wyU7wpn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# install svgling for drawing trees with NLTK in ipynb files\n",
        "!pip install svgling\n",
        "import svgling\n",
        "# the following will allow us to create a list or row of trees and then draw them, possibly captioned\n",
        "from svgling.figure import RowByRow, SideBySide, Caption"
      ],
      "metadata": {
        "id": "nhY80Dvt5_m1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WOZjh3mh5i1a"
      },
      "source": [
        "And now, we will make a trivial context free grammar just to ensure that the tech is working.  It will only be able to parse the sentence \"I left\" and will do so by saying that sentences are made of a NP, \"I\", and a VP, \"left\".  We'll define it, draw a tree with it, and then move on to more elaborate grammars.\n",
        "\n",
        "The command below defines a string containing three lines, each line with a rule of our context free grammar.\n",
        "\n",
        "> The triple-quotation-mark syntax allows you to have a string that spans multiple lines."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ijMIoPbA5i1a"
      },
      "outputs": [],
      "source": [
        "gramileft = \"\"\"\n",
        "S -> NP VP\n",
        "NP -> 'I'\n",
        "VP -> 'left'\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8qP-EjW5i1b"
      },
      "source": [
        "Having defined the string, we feed it to NLTK to turn it into a proper Context Free Grammar object."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NuRyz1LO5i1b"
      },
      "outputs": [],
      "source": [
        "cfgileft = nltk.CFG.fromstring(gramileft)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ymVdzma5i1b"
      },
      "source": [
        "> *Convention*: In this notebook, we will designate grammars with names like `ileft` and things related to those grammars with prefixes.  So:\n",
        ">\n",
        "> - `gram...` (e.g. `gramileft`) is the string that specifies a context free grammar.\n",
        "> - `cfg...` (e.g. `cfgileft`) is the context free grammar object that NLTK creates from them.\n",
        "> - `parser...` (e.g. `parserileft`) is a parser created to handle a given context-free grammar.\n",
        "> - `gen...` (e.g. `genileft`) is a parse-generator created by the parser for a given sentence.\n",
        ">\n",
        "> We will continually define and redefine `raw` (raw text), `sent` (tokenized text), `parses` (parses generated by a parse-generator), `trees` (trees, representing parses)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uI0hGqM25i1b"
      },
      "source": [
        "We can see that it worked by telling this grammar object to list the things it can produce.  It should give you a list of the rules back."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hYjGf1MV5i1c"
      },
      "outputs": [],
      "source": [
        "cfgileft.productions()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4nDueFI55i1c"
      },
      "source": [
        "Now, let's define the sentence we would like to have our grammar parse.  We will put this sentence in the variable `raw` (for raw text).  Then, we will define `sent` (sentence) as being the list of words in our raw text, by using `.split()` to split on \"whitespace\" and collect the results in a list.  This process of splitting a text into words like this is called \"tokenizing.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iPruNdFY5i1c"
      },
      "outputs": [],
      "source": [
        "raw = \"I left\"\n",
        "sent = raw.split()\n",
        "print(sent)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VI9sTFAP5i1c"
      },
      "source": [
        "We now ask NLTK to make a parser just for our little toy grammar.  The parser type will be a recursive descent parser, tailored to parse based on the `cfgileft` grammar.  We will call our parser `parserileft`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-VrDCvZ55i1c"
      },
      "outputs": [],
      "source": [
        "parserileft = nltk.RecursiveDescentParser(cfgileft)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dv4s0ETS5i1d"
      },
      "source": [
        "Now that we have our parser, we can tell it to parse something, specifically `sent`.  What we get back (and assign the name `genileft` to) is a \"generator\", which is an object that generates the next tree until we are out of possible parses.\n",
        "\n",
        "A generator isn't a list of parses, it's a procedure that can produce a list of parses.  One way to make it produce the list is to iterate through it, another is to coerce it into a list by using `list()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WYef2xlm5i1d"
      },
      "outputs": [],
      "source": [
        "genileft = parserileft.parse(sent)\n",
        "print(genileft) # reveals that this is a parse-generator, not yet actual parses"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MMiUzwgO5i1d"
      },
      "source": [
        "We will use the generator by iterating through it.  Below `for t in genileft` will iterate through the parses this generator can find, and will `print()` each parse in turn.\n",
        "\n",
        "Note that when you use a generator, it gets \"consumed.\"  More technically, the generator knows how to move to the next thing in the list of things it is going to generate, but it does not know how to go back to a previous thing.  So, it can only move forward, and so if you ask it to keep generating more trees until it runs out, it will then be out, and asking it again will not yield anything.  The little code vignette below shows this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AL_t8W0w5i1d"
      },
      "outputs": [],
      "source": [
        "print('Defining genileft as a parse-generator for sent.')\n",
        "genileft = parserileft.parse(sent)\n",
        "# first time through we get some trees\n",
        "print('parse(s) from genileft, first time used:')\n",
        "for t in genileft:\n",
        "    print(t)\n",
        "# second time \"through\" we get nothing because we had already reached the end of the parses\n",
        "print('parse(s) from genileft, second time used:')\n",
        "for t in genileft:\n",
        "    print(t)\n",
        "# \"third\" time through after having redefined the generator, gives us the trees again\n",
        "print('Defining genileft as a parse-generator for sent.')\n",
        "print('parses(s) from genileft, third time used after redefining it:')\n",
        "genileft = parserileft.parse(sent)\n",
        "for t in genileft:\n",
        "    print(t)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8cMlkmfe5i1e"
      },
      "source": [
        "The only way to get the trees back is to redefine the generator.  So, we'll do that now, but then we'll collect the generated trees into a list of trees.  Once they are stored in the list, we can refer to them as many times as we'd like.  We consume the generator making the list, but then we can refer to the list after that."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m9bhgFTk5i1e"
      },
      "outputs": [],
      "source": [
        "genileft = parserileft.parse(sent)\n",
        "parses = list(genileft)\n",
        "print(parses)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FWTYRCPF5i1e"
      },
      "source": [
        "Now, let's draw our tree.\n",
        "\n",
        "> This is the part where installing `svgling` helps us.  If you had not done that, you will probably get an error here, as it attempts to open a new window and draw a tree in it.  Note that although NLTK standard practice would be to call `draw()` on the tree, we do not want to do that here in a Colab notebook.  We will instead just let Colab reveal the tree's contents on its own terms.\n",
        "\n",
        "To draw the tree, just let Python try to display the contents.  That's the simplest way to do this."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "parses[0]"
      ],
      "metadata": {
        "id": "kYqQ0suN6e8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jDoBjJe-5i1f"
      },
      "source": [
        "# Developing a more complex phrase structure grammar\n",
        "\n",
        "We can start with the basic \"park grammar\" that comes from the NLTK book (so named I guess because it handles sentences that contain \"in the park\").\n",
        "\n",
        "The first part of the grammar specification below generally is defining the possible structures of sentences in general, and then the latter part of the grammar specification is defining the words.  It is possible to use the pipe character (`|`) to separate disjunctive options (essentially like a logical \"or\").  The grammar given below is what we want.  It shows that there are three verbs, four prepositions, four determiners, five nouns, three proper names.  It shows that verbs can either be followed by an NP (the object) or an NP and a PP (an object and a prepositional indirect object).  Prepositions are followed by an object.  And NPs can either be a name, or be a Det plus an N and optionally plus a prepositional phrase.\n",
        "\n",
        "```python\n",
        "S -> NP VP\n",
        "VP -> V NP | V NP PP\n",
        "PP -> P NP\n",
        "V -> 'saw' | 'ate' | 'walked'\n",
        "NP -> 'John' | 'Mary' | 'Bob' | Det N | Det N PP\n",
        "Det -> 'a' | 'an' | 'the' | 'my'\n",
        "N -> 'man' | 'dog' | 'cat' | 'telescope' | 'park'\n",
        "P -> 'in' | 'on' | 'by' | 'with'\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YDswvbEK5i1f"
      },
      "source": [
        "## TASK 1 define `grampark`\n",
        "\n",
        "Put this grammar's specification in a string called `grampark`, following the same procedure we used for `gramileft` above.\n",
        "\n",
        "> This is not supposed to be hard.  You can copy the grammar from the cell above, and paste it into a definition of a multi-line string called `grampark`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L1ItcsXP5i1f"
      },
      "outputs": [],
      "source": [
        "# Answer 1: define a multi-line string called grampark with the grammar shown above\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lB7bHuU-5i1f"
      },
      "source": [
        "## TASK 2 define `cfgpark`\n",
        "\n",
        "Have NLTK build a CFG object out of this specification, call it `cfgpark`, following the model of creating `cfgileft` above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VqMBTmXt5i1f"
      },
      "outputs": [],
      "source": [
        "# Answer 2: define a CFG called cfgpark from grampark\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eYtee9B25i1g"
      },
      "source": [
        "In preparation for the next task, we will parse the sentence \"Bob saw a telescope on a dog\" and count how many parses it has.  (It has 2, because it is two-ways ambiguous.)  This serves as an example of what you will do in Task 3.  And a test to be sure that you've got the steps above correct.\n",
        "\n",
        "> The code below loops through the numbers representing how many parses the sentence had (`len(trees)`) rather than simply looping through the parses as we did up in the code vignette above.  My only reason for doing it that way was so I could print the labels \"Parse 1\" and \"Parse 2\" using the number of the current parse.\n",
        ">\n",
        "> To draw trees side by side, follow the recipe below.  Start with an empty list (`trees = []`), use `svgling.draw_tree()` to draw the tree for a parse and assign it a name (`tree`), and then `append` the tree to the list.  Here, we wrapped the tree in `Caption(tree, string)` to add a caption below the tree.\n",
        ">\n",
        "> At the end, when all the side-by-side trees are in the list, call `SideBySide(*trees)` -- note the asterisk before `trees`.  This is necessary, it basically feeds the trees one by one to `SideBySide()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E8_sfu9l5i1g"
      },
      "outputs": [],
      "source": [
        "parserpark = nltk.RecursiveDescentParser(cfgpark)\n",
        "raw = \"Bob saw a telescope on a dog\"\n",
        "sent = raw.split()\n",
        "genpark = parserpark.parse(sent)\n",
        "parses = list(genpark)\n",
        "print(\"{}. {} parse(s)\".format(raw, len(parses)))\n",
        "trees = []\n",
        "for i in range(len(parses)):\n",
        "  tree = svgling.draw_tree(parses[i])\n",
        "  trees.append(Caption(tree, \"Parse {}\".format(i+1)))\n",
        "SideBySide(*trees)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DXjSMLcN5i1g"
      },
      "source": [
        "## TASK 3 test `parserpark`\n",
        "\n",
        "Show how many parses `cfgpark` can generate for each of the following sentences:\n",
        " - Mary saw Bob\n",
        " - my dog saw a cat in the park\n",
        " - a dog ate John\n",
        " - Mary saw\n",
        "\n",
        "To do this, follow the process we used earlier.  Specifically, you'll need to have NLTK make a parser object for you, then tell that parser object to parse each of the (tokenized) sentences above, and then print the length of the list of parses each sentence gets.  You don't need to draw the trees, just count the number of parses.\n",
        "\n",
        "> You should get 1, 2, 1, and 0 parses for the four sentences above, respectively, if it worked.\n",
        ">\n",
        "> You can of course follow the logic of the code we used above, that is why it is there.\n",
        "> But you do not need to redefine `parserpark`, since it is already defined by the code above\n",
        "> and does not need to change (since we are still working with the same grammar)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aJoIn4_O5i1g"
      },
      "outputs": [],
      "source": [
        "# Answer 3. Parse the sentences above and print how many parses each gets.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "naICFsv75i1g"
      },
      "source": [
        "## TASK 4 `grampark2`: adjectives and PPs\n",
        "\n",
        "In this task, we will create a grammar specification called `grampark2` that extends `grampark` to allow for (multiple) adjectives and multiple PP modifiers.\n",
        "Specifically, the goal is for it to be able to handle\n",
        "\"my annoying little dog saw a cat by Bob in the lovely park\"\n",
        "and\n",
        "\"a vicious dog ate John.\"\n",
        "\n",
        "To complete this task, just define `grampark2`, which is a multi-line string that defines a context free grammar.  We will use this in the activities below to test it out.  For more step-by-step hints, read the indented text below.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T2f3ymIM5i1g"
      },
      "source": [
        "> The NLTK book (chapter 8, around example 3.3) works through the adjective part of this, though it is very concise there.\n",
        ">\n",
        "> When you think about the structure of \"my annoying little dog\", we have a determiner (*my*),\n",
        "> two adjectives (*annoying* and *little*), and then the head noun (*dog*).  So, in addition to\n",
        "> `NP -> Det N` (which we need to keep so we can still get \"a dog\"), we need an `NP` rule that\n",
        "> allows for one or two adjectives between `Det` and `N`.\n",
        ">\n",
        "> There are two approaches here.  One is to create a grammar that allows for 0, 1, or 2 adjectives between a determiner and a noun.  That can be accomplished by adding two more rules.  But then if we ever need to handle a sentence containing \"my annoying little fluffy dog\" we'd need to add yet another rule.\n",
        ">\n",
        "> The other approach is suppose that we can have essentially any number of adjectives between the Det and the N.  The way to do this is to create a recursive rule, one that will add an adjective to something and yield something that it suitable for adding another adjective to.  In the NLTK book, a \"Nom\" category is proposed, which is essentially the same as what might have been called \"N-bar\" in Intro to Linguistics.  For our purposes, the simplest version of this would be:\n",
        ">\n",
        "> ```\n",
        "N -> Adj N\n",
        "```\n",
        "> This expands our understanding of \"N\" from \"a word that is a noun\" to \"one or more words that function as a noun does\", but it gets what we want.  Specifically, if you have Adj \"fluffy\" and N \"dog\", you can make the N \"fluffy dog\", which you can then add the Adj \"little\" to, yielding \"little fluffy dog\", etc.\n",
        ">\n",
        "> While we're at it, PP nodes attached to Ns behave in pretty much the same way.  You can have any number of them, suggesting that they too are a good candidate for a recursive rule.\n",
        ">\n",
        "> ```\n",
        "N -> N PP\n",
        "```\n",
        "> However, when you add that rule above, you want to remove the rule that added a single PP within the NP (so that there is only one rule, the one just above here, that introduces a PP within an NP).\n",
        ">\n",
        "> In fact, let's not stop there.  The VP can also have multiple PPs attached to it, so let's revise the VP rule so that PP can modify verbs even when those verbs have no object.  That is, let's add:\n",
        ">\n",
        "> ```\n",
        "VP -> VP PP\n",
        "```\n",
        ">So, let's add those three recursive rules above (two for N, one for VP), as well as the rules that define the adjectives:\n",
        ">```\n",
        "Adj -> 'annoying' | 'little' | 'lovely' | 'vicious' | 'fluffy'\n",
        "```\n",
        ">\n",
        "> And don't forget to *remove* the rules that we had introducing PPs before (remove the rule rewriting NP to Det N PP, and the rule rewriting VP to V NP PP).\n",
        ">\n",
        "> To define the multi-line string `grampark2`, you can just copy and paste the `grampark` string (from Task 1) and then\n",
        "> modify it as needed to add support for adjectives and the recursive N rules described above.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RF-FGBrn5i1g"
      },
      "outputs": [],
      "source": [
        "# Answer 4: Define a multi-line string gramparkadj that specifies the grammar, extended to handle adjectives\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bNFjIGiC5i1h"
      },
      "source": [
        "Now we will build the CFG object from this description, and have NLTK make a parser based on that CFG.\n",
        "\n",
        "**NOTE** We need to use a `ChartParser` now rather than a `RecursiveDescentParser` because the recursive CFG we have created will send a recursive descent parser into an infinite loop."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tDWoI6wc5i1h"
      },
      "outputs": [],
      "source": [
        "cfgpark2 = nltk.CFG.fromstring(grampark2)\n",
        "# we cannot use nltk.RecursiveDescentParser(cfgpark2) with this kind of recursive CFG\n",
        "parserpark2 = nltk.ChartParser(cfgpark2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tlUzGyp55i1h"
      },
      "source": [
        "If it worked, the following code should succeed.  (Look through the code to see what it is doing,\n",
        "but you should get the \"Success!\" message.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rfUfQeTS5i1h"
      },
      "outputs": [],
      "source": [
        "raw = 'Mary saw a lovely cat in the park'\n",
        "sent = raw.split()\n",
        "genpark2 = parserpark2.parse(sent)\n",
        "parses = list(genpark2)\n",
        "if len(parses) > 0:\n",
        "    print('\\o/ Success! The sentence was parsed (into {} parses).'.format(len(parses)))\n",
        "else:\n",
        "    print(\"D'oh! Something is amiss.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's look a bit at what the actual parses were.  There are three."
      ],
      "metadata": {
        "id": "0LEo5c1Ad6KX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trees = [svgling.draw_tree(t, font_size=12) for t in parses]\n",
        "SideBySide(*trees)"
      ],
      "metadata": {
        "id": "0F3_xqfmXVyU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Intuitively, this seems like it might be too many parses, but looking at the trees, we can see what happened. There are three places the PP *in the park* can be attached. It can be attached to the VP (the seeing was in the park), or it can be attached to the NP (the cat was in the park).  However, because there is an adjective (*lovely*) as well, the PP can either attach below the adjective (it is the *cat in the park* that is *lovely*) or above it (it is the *lovely cat* that is *in the park*).  This (arguably) may not make a difference to the meaning, but the structures are still distinct.\n",
        "\n",
        "And it'll get worse if you have two adjectives and two PPs.  Try *Mary saw an annoying little cat with a telescope in the park.* How many parses does that sentence have?  Yikes.\n"
      ],
      "metadata": {
        "id": "WqToBIFkYfw-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "raw = 'Mary saw an annoying little cat with a telescope in the park'\n",
        "sent = raw.split()\n",
        "genpark2 = parserpark2.parse(sent)\n",
        "parses = list(genpark2)\n",
        "if len(parses) > 0:\n",
        "    print('\\o/ Success! The sentence was parsed (into {} parses).'.format(len(parses)))\n",
        "else:\n",
        "    print(\"D'oh! Something is amiss.\")"
      ],
      "metadata": {
        "id": "aLJn6pOTZfdY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N2_EFNDb5i1h"
      },
      "source": [
        "Let's define a function that can take a\n",
        "string, break it up into words, parse it, and return the trees.  That will make\n",
        "it simpler to deal with this procedure.\n",
        "Take a moment to understand the code, and see how it relates to what we just did."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_rXV9-_i5i1h"
      },
      "outputs": [],
      "source": [
        "def get_parses(raw, cfg):\n",
        "    sent = raw.split()\n",
        "    parser = nltk.ChartParser(cfg)\n",
        "    treegen = parser.parse(sent)\n",
        "    parses = list(treegen)\n",
        "    return parses"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "meIw6hQm5i1i"
      },
      "source": [
        "Now, we can do the same check we did above, but making use of our `get_trees()` function.  You should still get the \"Success!\" message.  Look it over to understand what is happening."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5SRhQMD75i1i"
      },
      "outputs": [],
      "source": [
        "parses = get_parses('Mary saw a lovely cat', cfgpark2)\n",
        "if len(parses) > 0:\n",
        "    print('\\o/ Success! The sentence was parsed (into {} parses).'.format(len(parses)))\n",
        "else:\n",
        "    print(\"D'oh! Something is amiss.\")  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JqO05-PP5i1i"
      },
      "source": [
        "## TASK 5 testing `get_parses`\n",
        "\n",
        "Show how many parses the grammar specified by `gramparkadj` gives (as you did in Task 3) for the following sentences:\n",
        " - my annoying little dog saw a cat in the lovely park\n",
        " - a vicious dog ate John\n",
        " - a man walked in the park\n",
        "\n",
        "> You should get 2, 1, and 0, respectively, if it worked."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s9GEuhDt5i1i"
      },
      "outputs": [],
      "source": [
        "# Answer 5: Parse the sentences above with cfgparkadj and print how many parses each gets\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lINX5FP-5i1i"
      },
      "source": [
        "## TASK 6 walking in the park\n",
        "\n",
        "This grammar will give you nothing for \"a man walked in the park\".  Intuitions about English suggest that this should be possible. What is it about `grampark2` that leads this not to be grammatical/parsable?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rX7hrVC85i1i"
      },
      "source": [
        "**Answer 6** (markdown)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b_iVTIo35i1i"
      },
      "source": [
        "# Traversing trees, finding subjects and objects"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ig1doRo05i1i"
      },
      "source": [
        "Next, we will try to find the subject of a sentence.  Descriptively, the subject\n",
        "of a sentence is the NP that is a daughter of S.  Ultimately, in this grammar we have built\n",
        "so far, it's always going to be in the same place, but let's explore this a little bit anyway.\n",
        "\n",
        "We can take a tree that our parser has found for us and break it up into subtrees, which\n",
        "will allow us to isolate NP-daughter-of-S pretty easily.  So, for the \"John saw Mary\" tree,\n",
        "what `get_parses(\"John saw Mary\")` gives us back is a list of parses (containing just one\n",
        "element), so let's look at that parse.  We'll name it `parse1`.  Good day to you, `parse1`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gqcK3ptK5i1j"
      },
      "outputs": [],
      "source": [
        "parse1 = get_parses(\"John saw Mary\", cfgpark2)[0]\n",
        "parse1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o7zthdQ_5i1j"
      },
      "source": [
        "We can ask things of type `Tree` to provide `subtrees()`, which will give us *all* the subtrees contained in it."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "list(parse1.subtrees())"
      ],
      "metadata": {
        "id": "Fgit6YE1eH-e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we are looking for NP-daughter-of-S, we first want to find the Ses, and then we can look at the daughters to find an NP.  In this case, we have just the one S, but later we will look at more complex sentences where one S in contained inside another.  So, let's do this in a general way from the beginning."
      ],
      "metadata": {
        "id": "oTWvypz9eazz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qkzz-6RW5i1j"
      },
      "outputs": [],
      "source": [
        "# find all the subtrees labeled \"S\"\n",
        "ssubtrees = [n for n in parse1.subtrees() if n.label() == 'S']\n",
        "# go through each S and find the NP daughters\n",
        "subjects = [d for snode in ssubtrees for d in snode if d.label() == 'NP']\n",
        "# report on what we found\n",
        "print(subjects)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vb16Wze25i1l"
      },
      "source": [
        "We can actually combine these in a single (though complicated) list comprehension:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KOS8LE545i1l"
      },
      "outputs": [],
      "source": [
        "[d for n in parse1.subtrees() if n.label() == 'S' for d in n if d.label() == 'NP']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U1sxgfeV5i1l"
      },
      "source": [
        "> This takes the NP-finder above, but adds in the computation of `snodes` as well. Notice the order.  We're making a list of `d`s, which are the daughters of `snode`. So we start with `[d for...` but then we are going to find the `snodes` first, and then the daughters of those once we have an `snode`.  So, we continue with `n in parse1.subtrees() if n.label() == \"S\"` meaning that `n` is going to be a subtree with label \"S\" that we want to then check the daughters of.  So, then we go through the daughters with `for d in n if d.label() == \"NP\"`.  Put together, it looks as given above.\n",
        ">\n",
        ">Saying it again/slightly differently: To read this out loud in something like English: skip the `[d for` part until the end: \"For each node `n` in whose label is `\"S\"`, and for each node `d` in `n` whose label is `\"NP\"`, add `d` to the list\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we make some assumptions about the grammar (in particular, that the first daughter of S is always going to be the subject), we can do the same thing by just gathering the first daughters of all the Ses (without bothering to check for an NP label)."
      ],
      "metadata": {
        "id": "V656IzQtgzko"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OiN_Ny2O5i1k"
      },
      "outputs": [],
      "source": [
        "[snode[0] for snode in ssubtrees]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "euhLndbQ5i1l"
      },
      "source": [
        "## TASK 7 Locate the object\n",
        "\n",
        "Understand how that complex list comprehension works.\n",
        "It's not simple.  Even I have to stare at these for a little while before I get it.\n",
        "Re-read the explanation above a couple of times and keep in mind what this is supposed\n",
        "to be accomplishing.\n",
        "Then, **convince yourself\n",
        "that you have succeeded\n",
        "by changing it so that it finds the object instead.**\n",
        "(What we did above is find the subject, which is the\n",
        "NP daughter of S.  So, how do we characterize the object?  Use the technique above to find it.\n",
        "The answer should be \"Mary\", right?)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "98ZOHiJm5i1l"
      },
      "outputs": [],
      "source": [
        "# Answer 7: Revise the list comprehension above to find the object instead.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fptoCowo5i1l"
      },
      "source": [
        "Now, let's enhance the grammar by adding the ability to embed clauses, like in\n",
        "\"Bob thought that John saw Mary\" and \"Bob said that John saw Mary\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4NTaNOZL5i1l"
      },
      "source": [
        "## TASK 8 Add embedded sentences\n",
        "\n",
        "Enhance the grammar so that it can parse \"Bob thought that John saw Mary\" and \"Bob said that John saw Mary\".  Call the (multi-line string) specification of the new grammar `gramcomp` and have NLTK create a CFG object from it called `cfgcomp`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7X0b16QE5i1m"
      },
      "source": [
        "> The idea here is to add still more to `grampark2` from Task 4.  We need to add the verbs `\"said\"` and `\"thought\"` at least, and the\n",
        "> complementizer `\"that\"`.  So to start, just copy and paste the definition of `grampark2` from Task 4, and then revise it.  We'll call the reviesed version `gramcomp`.\n",
        "> \n",
        "> Consider that, although \"said\" and \"thought\" are verbs, they do not take NP objects.\n",
        "> So they're a different kind of a verb.  They are in the *category* of \"verb\" but they\n",
        "> are a sub-type, a *sub-category* of verb.  So, we do not simply want to add something\n",
        "> like `... | \"thought\" | \"said\"` to the `V ->` line.  We need a different kind of verb,\n",
        "> the book calls them \"Sentential verbs\" and gives them a label of `SV`, so we can follow\n",
        "> that here.\n",
        ">\n",
        ">```\n",
        "SV -> 'said' | 'thought'\n",
        "> ```\n",
        "> \n",
        "> If the sentential verbs are category `SV`, we still want to be able to form a `VP`\n",
        "> out of a `SV` and its complement.  So, we need to add that as an option to the `VP`\n",
        "> rules.  In order to do this, we also need to figure out what the complement of such\n",
        "> a verb is.\n",
        "> \n",
        "> This is simplifying things, but let's assume that the complement of \"thought\" is\n",
        "> basically always \"that S\" --- so \"that\" is a complementizer, we can call it category\n",
        "> `C` and we can form a `CP` from `C` and `S`.  Then `SV` type verbs will have a\n",
        "> `CP` as their complement.  It's pretty close to what you'd have seen in Intro\n",
        "> syntax, apart from probably calling `S` \"IP\" instead.\n",
        ">\n",
        ">```\n",
        "VP -> SV CP\n",
        "CP -> C S\n",
        "C -> 'that'\n",
        "> ```\n",
        "> \n",
        "> TL;DR: You will want to add rules with left sides being CP, C, SV, and another with VP."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bhwB32555i1m"
      },
      "outputs": [],
      "source": [
        "# Answer 8: Define gramcomp and cfgcomp\n",
        "# (allowing sentential complements of thought and said)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ddu1-Jt5i1m"
      },
      "source": [
        "## TASK 9 Two complex trees\n",
        "\n",
        "Give trees for:\n",
        " - Bob said that John saw Mary in the park\n",
        " - the annoying man thought that Bob said that my dog saw a vicious cat in the park\n",
        " \n",
        "You can use the text representations of the trees that `print(tree)` provides.  Also: don't forget that we defined `get_parses(raw, cfg)` above, so you can just use that as-is to get the parses.\n",
        "\n",
        "Print the tree for the first parse of each (don't bother printing trees of all the possible parses of the second one)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lh5k-we15i1m"
      },
      "outputs": [],
      "source": [
        "# Answer 9: Print trees for the first parse of sentences\n",
        "# 9a: Bob said that John saw Mary in the park\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 9b: the annoying man thought that Bob said that my dog saw a vicious cat in the park\n",
        "\n"
      ],
      "metadata": {
        "id": "rdjy_EgZlaxb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KOV56MEw5i1m"
      },
      "source": [
        "## TASK 10 Locate many subjects\n",
        "\n",
        "Find the subjects of those sentences using our subject-finding procedure\n",
        "from before.\n",
        "It should be \"Bob\" and \"John\" in one case, \"the annoying man\", \"Bob\", and \"my dog\" in the other.\n",
        "(Also, it is ok if your subjects when printed look like `[Tree('NP', ['Bob'])]` rather than just \"Bob\".)\n",
        "\n",
        "> Here, you would just use the list comprehension for finding subjects above Task 7."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3XHP4M_b5i1m"
      },
      "outputs": [],
      "source": [
        "# Answer 10: Find (and print) the subjects of the sentences above\n",
        "# first sentence\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# second sentence\n"
      ],
      "metadata": {
        "id": "MYqKKafrn1jj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Q47zC2m5i1m"
      },
      "source": [
        "## TASK 11 Locate many objects\n",
        "\n",
        "Find the objects of those sentences using our\n",
        "object-finding procedure from before.  (Should be \"Mary\" in one case, \"a vicious cat (in the park)\" in the other.)\n",
        "\n",
        "> Here, you can just use the object-finder from Task 7."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1H3ZpY-h5i1m"
      },
      "outputs": [],
      "source": [
        "# Find (and print) the objects of the sentences above\n",
        "# first sentence\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# second sentence\n"
      ],
      "metadata": {
        "id": "XJ2MUdYXobXG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DxsAEoTY5i1m"
      },
      "source": [
        "# Relative clauses\n",
        "\n",
        "A relative clause is something like \"who saw Mary\" in \"the man who saw Mary\". \n",
        "It is formed by adding a *wh*-question to a noun, more or less.  So the referent\n",
        "of \"the man who saw Mary\" is the individual that is a man, and also the answer\n",
        "to the question \"Who saw Mary?\". \n",
        "\n",
        "Suppose we want our parser to recognize \"the man who saw Mary\" as an NP.\n",
        "\n",
        "It can already recognize \"the man\" and \"the man in the park\", so we can\n",
        "simply add an extra option for the `NP` rule to allow for this."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3HHjoCmP5i1m"
      },
      "source": [
        "In \"the man who saw Mary\", it seems like \"who\" is basically the\n",
        "subject of \"saw\".  So, \"who saw Mary\" is a special kind of sentence\n",
        "with \"who\" as the subject.  Let's define this kind of special case\n",
        "by, first, making \"who\" a special kind of NP, and then making a\n",
        "relative clause be a special kind of sentence with \"who\" as its\n",
        "subject.\n",
        "\n",
        "So, we can add these to the grammar (`RP` is the relative pronoun, `RC` is the relative clause, which is a relative pronoun and a verb phrase, and we add one more kind of `NP` that has a `RC` attached)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sQzfs2oV5i1m"
      },
      "outputs": [],
      "source": [
        "gramrcsub = gramcomp + \"\"\"\n",
        "RP -> 'who'\n",
        "RC -> RP VP\n",
        "NP -> Det N RC\n",
        "\"\"\"\n",
        "\n",
        "cfgrcsub = nltk.CFG.fromstring(gramrcsub)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I-jvJQU75i1n"
      },
      "outputs": [],
      "source": [
        "parses = get_parses(\"the man who saw Mary saw Bob\", cfgrcsub)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FtPWXD4h5i1n"
      },
      "outputs": [],
      "source": [
        "parses[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nyM45QM75i1n"
      },
      "source": [
        "There's another form a relative clause can take, though.  You can also say\n",
        "\"the man who Mary saw saw Bob\".  What's different here is that \"who\" is now \n",
        "playing the role of the object, rather than the subject.\n",
        "\n",
        "The relative pronoun \"who\" generally corresponds to a gap\n",
        "in the sentence.  We didn't notice the gap before, when the gap was in the subject\n",
        "position, but it's obvious\n",
        "when the gap is in the object position.  These relative clauses are, again,\n",
        "basically *wh*-questions, and the normal way *wh*-questions are formed is to \n",
        "move the *wh*-word to the front of the clause.\n",
        "\n",
        "And this is where parsing becomes difficult, when things move around in a sentence.\n",
        "\n",
        "Let's try a kind of a hack to make this work.\n",
        "\n",
        "For any transitive verb (\"saw\", \"ate\", and \"walked\" in our grammar), there is the\n",
        "version we already have, which form a VP with their object NP.  If any of these appear\n",
        "in an \"object relative\", then the object NP will be \"missing\".  So, let's make a version\n",
        "of the VP that has a \"**gap**\".  That is, we will define `VPG` (VP-gap) to be just `V` rather than \n",
        "`V NP`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FNd_QTkj5i1n"
      },
      "source": [
        "The line below will give us a list of all the rules in `cfgrcsub` that have `VP` on the left side. For any of these that have `V NP` in them, we want to make a `VPG` version (a VP with an object gap) with the `NP` omitted."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6R3WOT425i1n"
      },
      "outputs": [],
      "source": [
        "[p for p in cfgrcsub.productions() if str(p.lhs()) == 'VP']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xYmJ7Hbv5i1n"
      },
      "outputs": [],
      "source": [
        "gramvpg = gramrcsub + \"\"\"\n",
        "VPG -> V\n",
        "VPG -> V PP\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DZPBq4cJ5i1n"
      },
      "source": [
        "We haven't used `VPG` yet in the grammar apart from defining it.  But conceptually,\n",
        "what we want is that `VPG` should be available in a relative clause where the\n",
        "object is missing.  So, we want to add an expansion for `RC`, like this:\n",
        "```python\n",
        "RC -> RP SWOG\n",
        "SWOG -> NP VPG\n",
        "```\n",
        "The idea here is that `SWOG` (sentence-with-object-gap) is like a regular `S` but\n",
        "has a `VP` with an object gap."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_d7VZ1U5i1n"
      },
      "source": [
        "To do this replacement, it's probably easier to edit the whole specification.  So, let's print out the multi-line string that specifies our most recent version of the grammar, and then afterwards you can copy and paste that into the next version of the grammar, making the changes just mentioned (adding `SWOG` in)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RsUZ6GYu5i1n"
      },
      "outputs": [],
      "source": [
        "print(gramvpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xVUX88q65i1o"
      },
      "source": [
        "## TASK 12 object relatives\n",
        "\n",
        "Define a multi-line string `gramrcobj` that specifies a grammar that can handle object relative clauses, by copying the grammar above and then adding the modifications for `SWOG` discussed above that."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TFpC9jpr5i1o"
      },
      "outputs": [],
      "source": [
        "# Answer 12: define gramrcobj as above but with modifications for SWOG.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ML8BIN7y5i1o"
      },
      "source": [
        "Assuming you did that right, we should now get a tree below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bF9-Ybov5i1o"
      },
      "outputs": [],
      "source": [
        "cfgrcobj = nltk.CFG.fromstring(gramrcobj)\n",
        "parses = get_parses(\"the man who Mary saw in the park saw Bob\", cfgrcobj)\n",
        "parses[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PiXQMwL25i1o"
      },
      "source": [
        "Having analyzed object relatives this way really probably means that we should re-analyze subject relatives to match (so that there is a gap even in subject relatives).  We won't bother with that here though."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8MYqwAdt5i1o"
      },
      "source": [
        "## TASK 13 the man who saw the man who Mary saw\n",
        "\n",
        "Draw a tree for \"the man who saw the man who Mary saw saw Bob\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V6urUcWQ5i1o"
      },
      "outputs": [],
      "source": [
        "# the man who saw the man who Mary saw saw Bob\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W_RmIuZg5i1p"
      },
      "source": [
        "## Advanced/optional: TASK 14 Find subjects and objects\n",
        "\n",
        "Define a function `find_subjects` that will find subjects and a function `find_objects` that will find objects, both of which will work with relative clauses.  Specifically, the subjects of the tree above should be \"the man who was the man who Mary saw\", \"who\", \"Mary\"; the objects should be \"Bob\", \"the man who Mary saw\", \"who\".  This will require looking not only at NP daughters of VP and S but looking also at RCs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a-K2Mt5n5i1p"
      },
      "outputs": [],
      "source": [
        "# Answer 14: define find_subjects(raw, cfg) and find_objects(raw, cfg)\n",
        "# so that they will find subjects/objects in sentences with relative clauses too\n",
        "\n",
        "def find_subjects(raw, gram):\n",
        "  # revise this to actually return the right things\n",
        "  return ['a', 'b']\n",
        "\n",
        "def find_objects(raw, gram):\n",
        "  # revise this to actually return the right things\n",
        "  return ['a', 'b']\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code below should test your functions."
      ],
      "metadata": {
        "id": "twq6XzoD4cQ_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "subjs = find_subjects('the man who saw the man who Mary saw saw Bob', cfgrcobj)\n",
        "print(\"Found {} subjects, should be 3.\".format(len(subjs)))\n",
        "print(\"Last two should be 'who' and 'Mary' and are:\")\n",
        "print(subjs[-2])\n",
        "print(subjs[-1])"
      ],
      "metadata": {
        "id": "vaXegaTa1kJq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "objs = find_objects('the man who saw the man who Mary saw saw Bob', cfgrcobj)\n",
        "print(\"Found {} objects, should be 3.\".format(len(subjs)))\n",
        "print(\"Last two should be 'Bob' and 'who' and are:\")\n",
        "print(objs[-2])\n",
        "print(objs[-1])"
      ],
      "metadata": {
        "id": "849ffQMG5_zi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K_RVovGC5i1p"
      },
      "source": [
        "And that's it for the homework.  Feel free to play around with it, there are certainly more things one can do."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "nteract": {
      "version": "0.21.0"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}